---
layout: post
title: "국민청원 데이터셋 CSV"
description: 
headline: 
modified: 2019-02-03
category: 국민청원
tags: [국민청원, Dataset, crawling, CSV]
imagefeature: 
mathjax: 
chart: 
share: true
comments: true
---

국민청원 [데이터셋](https://www.dropbox.com/s/4q5neb9ngdhfg8y/petition_data_all.csv?dl=0)와 [크롤링 방법](https://github.com/newhiwoong/National-Petition/tree/master/Data)에 대해서 알아보겠다. 그냥 [데이터셋](https://www.dropbox.com/s/4q5neb9ngdhfg8y/petition_data_all.csv?dl=0)과 [브리핑 내용 데이터셋](https://www.dropbox.com/s/n8oj13ciji9x93h/briefing.csv?dl=0)을 다운로드 받고 나가도 된다.

## 크롤링
[Crawling.ipynb](https://github.com/newhiwoong/National-Petition/blob/master/Data/Crawling.ipynb) 내용이다.

아래 패키지들을 설치하고 
{% highlight yaml %}
pip install urllib
pip install bs4
pip install multiprocessing
{% endhighlight %}
---

필요한 패키지들을 import한다.
```python
from urllib.request import urlopen
from bs4 import BeautifulSoup
import urllib
import csv
import re
```
---

아래 코드로 크롤링을 진행할 예정이다. 참여인원은 `petitionsView_count class`로 제목은 `petitionsView_count class` 등으로 되어 있다. `try except`문을 사용하여 중간에 청원이 없어도 프로그램이 정지하지 않게 한다. 청원을 크롤링하면 해당 내용을 `CSV 파일`에 저장한다.

```python
def FindPetition(num):
    global kr, other
    url = "https://www1.president.go.kr/petitions/"
    url = url + str(num)
    try:
        html = urlopen(url)
    except urllib.error.HTTPError as e:
        print(e)
        print("=============error %d ============="%num)
    else:
        try:
            bsObj = BeautifulSoup(html.read(), "html.parser")
        except urllib.exceptions.SSLError as e:
            print(e)
            print("=============error %d ============="%num)
        else:
            #print("============%d===================" % num)
            progress = bsObj.find("div",{"class":"petitionsView_progress"}).get_text()
            title = bsObj.find("h3",{"class":"petitionsView_title"}).get_text()
            count = bsObj.find("h2",{"class":"petitionsView_count"}).get_text()
            petition_overview = bsObj.find("div",{"class":"View_write"}).get_text()
            days = bsObj.find("ul",{"class":"petitionsView_info_list"}).get_text()
            days = days.split("\n")
            category = days[1]
            sdays = days[2]
            edays = days[3]
            person = days[4]
            if(num%1000==0):
                print(num)
            #print("1. 청원 현황 : ", progress)
            #print("2. 청원 제목 : ",title)
            #print("3. 청원 참가자 수 : ",count)
            #print("4. 청원 내용 : ",petition_overview)
            
            try:
                days = str(days[4:])
                progress = str(progress[1:])
                title = str(title)
                count = str(count)
                petition_overview = str(petition_overview)
                category = str(category[4:])
                sdays = str(sdays[4:])
                edays = str(edays[4:])
                person = str(person[3:])
                
                #print(category,sdays,edays,person)
            except:
                print("=============error %d ============="%num)
            else:
                csvRow = []
                csvRow.extend([num,category,sdays,edays,person,progress,title,count,petition_overview])
                writer.writerow(csvRow)
```
---

그리고 위 함수를 아래 코드를 이용하여 [petition_data_all.csv](https://www.dropbox.com/s/4q5neb9ngdhfg8y/petition_data_all.csv?dl=0)파일에 내용을 저장하며 크롤링을 진행한다. 다행인 점은 국민청원의 URL이 순차적으로 돼 있다. 무슨 뜻이냐면 `https://www1.president.go.kr/petitions/32` 다음 청원은 `https://www1.president.go.kr/petitions/33` 이런 식으로 순서대로 되어 있어 `for`문을 통해서 간단하게 크롤링할 수 있다.
```python
csvFile = open("petition_data_all.csv", 'w', encoding='UTF-8')
writer = csv.writer(csvFile)
writer.writerow(["num","category","start-days","end-days","person","progress","title","count","petition_overview"])
for i in range(21,484869):
    FindPetition(i)
csvFile.close()
```
---

## 필터링
[Filtering.ipynb](https://github.com/newhiwoong/National-Petition/blob/master/Data/Filtering.ipynb)의 내용이다. 

크롤링을 진행했으나 참여인원이 `참여인원 : [ 1명 ]`형태로 되어 있어 사용하기가 불편하다. 이제 참여인원을 int형으로 바꾸는 작업을 진행할 것이다.

먼저 크롤링을 한 파일을 읽고
```python
df = pd.read_csv("../petition_data_all.csv")
df
```
---

아래 함수를 통해서 `참여인원 : [ 1명 ]`처럼 참여인원이 들어오면 `1`로 바꿔주는 함수를 작성하고 
```python
def find_int(count):
    i = (re.findall(r'\d+', count))
    ints = int(''.join(map(str, i)))
    return ints
```
---

`map함수`를 이용해서 모든 부분에 참여인원의 모든 부분을 바꾼다.
```python
df["count"] = df["count"].map(find_int)
df
```
---

그리고 다시 `petition_data_all.csv` 파일을 저장한다.
```python
df.to_csv("../petition_data_all.csv", index=None)
```
---

### 브리핑 내용 추가
progress가 브리핑인 내용의 형식이 약간 달라서 정부에서 브리핑한 청원을 다시 추가하는 작업이 필요하다. [브리핑 내용 데이터셋](https://github.com/newhiwoong/National-Petition/blob/master/Data/briefing.csv)

아래 코드로 브리핑을 한 내용들의 `num`을 찾는다.
```python
datas = df[df["progress"]=="브리핑 "]["num"]
```
---

위에서 일반 청원들을 크롤링한 것과 다르게 `class 형식`이 달라서`petition_all = bsObj.find("div",{"class":"petitionsView_write"}).get_text()`이 코드로 크롤링을 한 후 `청원개요`와 `답변원고`를 나눠서 저장한다.
```python
def FindPetition(num):
    global kr, other
    url = "https://www1.president.go.kr/petitions/"
    url = url + str(num)
    try:
        html = urlopen(url)
    except urllib.error.HTTPError as e:
        print(e)
        print("=============error %d ============="%num)
    else:
        try:
            bsObj = BeautifulSoup(html.read(), "html.parser")
        except urllib.exceptions.SSLError as e:
            print(e)
            print("=============error %d ============="%num)
        else:
            #print("============%d===================" % num)
            progress = bsObj.find("div",{"class":"petitionsView_progress"}).get_text()
            title = bsObj.find("h3",{"class":"petitionsView_title"}).get_text()
            count = bsObj.find("h2",{"class":"petitionsView_count"}).get_text()
            petition_all = bsObj.find("div",{"class":"petitionsView_write"}).get_text()
            petition_overview = petition_all[petition_all.find("청원개요")+4:petition_all.find("답변원고")]
            petition_answer = petition_all[petition_all.find("답변원고")+4:]
            
            
            days = bsObj.find("ul",{"class":"petitionsView_info_list"}).get_text()
            days = days.split("\n")
            category = days[1]
            sdays = days[2]
            edays = days[3]
            person = days[4]
            if(num%1000==0):
                print(num)
            #print("1. 청원 현황 : ", progress)
            #print("2. 청원 제목 : ",title)
            #print("3. 청원 참가자 수 : ",count)
            #print("4. 청원 내용 : ",petition_overview)
            #print("4. 청원 내용 : ",petition_answer)
            try:
                days = str(days[4:])
                progress = str(progress[1:])
                title = str(title)
                count = str(count)
                petition_overview = str(petition_overview)
                petition_answer = str(petition_answer)
                category = str(category[4:])
                sdays = str(sdays[4:])
                edays = str(edays[4:])
                person = str(person[3:])
                
                #print(category,sdays,edays,person)
            except:
                print("=============error %d ============="%num)
            else:
                csvRow = []
                csvRow.extend([num,category,sdays,edays,person,progress,title,count,petition_overview,petition_answer])
                writer.writerow(csvRow)
                
    return petition_overview
```
---

위에서 만든 `datas`변수 즉 브리핑한 내용만 가지고 다시 크롤링을 진행한다.
```python
csvFile = open("briefing.csv", 'w', encoding='UTF-8')
writer = csv.writer(csvFile)
writer.writerow(["num","category","start-days","end-days","person","progress","title","count","petition_overview","petition_answer"])
for i in datas:
    FindPetition(i)
csvFile.close()
```
---

똑같이 위에 `필터링` 작업을 진행하면 `petition_data_all.csv`와 같이 사용할 수 있다.